{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen  # b_soup_1.py\n",
    "from bs4 import BeautifulSoup\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Must run the following commands in the appropriate conda env in order for this to work:\n",
    ">  \\> conda install spacy  \n",
    ">  \\> python -m spacy download en   \n",
    " (second one must be run as admin)\n",
    "\n",
    "Alternatively run the top command and then replace the cell below with:  \n",
    "`import spacy  \n",
    "import en_core_web_sm  \n",
    "nlp = en_core_web_sm.load()`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# driver\n",
    "def do_that_thing(skills_list):\n",
    "    \"\"\"\n",
    "    Returns a dict mapping each skill in the skill_list\n",
    "    to a list of course names associated with that skill.\n",
    "    \"\"\"\n",
    "    # first get the links for all Heinz courses\n",
    "    course_dict = get_course_links()\n",
    "    course_descriptions = course_dict.copy()\n",
    "\n",
    "    # then scrape the description text for each link\n",
    "    for name, link in course_dict.items():\n",
    "        course_descriptions[name] = get_course_description(link)\n",
    "        \n",
    "    # time for a little nlp (to lemmatize the descriptions)\n",
    "    # we'll store the results in a new dictionary\n",
    "    parsed_descriptions = course_descriptions.copy()\n",
    "    for name, text in course_descriptions.items():\n",
    "        parsed_text = nlp(text)\n",
    "        parsed_tokens = []\n",
    "        \n",
    "        # remove all stopwords, punctuation, and spaces\n",
    "        for token in parsed_text:\n",
    "            lemma = token.lemma_.lower()\n",
    "            if not (nlp.vocab[lemma].is_stop or token.pos_ == 'PUNCT' or token.pos_ == 'SPACE'):\n",
    "                parsed_tokens.append(lemma)\n",
    "        \n",
    "        # finally add list to dictionary\n",
    "        parsed_descriptions[name] = parsed_tokens\n",
    "    \n",
    "    \n",
    "    # now we can iterate through the skills list and build our final dictionary\n",
    "    # give each skill its own list\n",
    "    skills_to_courses = {}.fromkeys(skills_list, [])\n",
    "    \n",
    "    for skill in skills_list:\n",
    "        # now check every course for that skill\n",
    "        for course_name in parsed_descriptions.keys():\n",
    "            # if that skill appears in the description, add to the map\n",
    "            if skill.lower() in parsed_descriptions[course_name]:\n",
    "                skills_to_courses[skill].append(course_name)\n",
    "    \n",
    "    # return the final mapping\n",
    "    return skills_to_courses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_course_links():\n",
    "    \"\"\"\n",
    "    Get a list of urls to Heinz College course description \n",
    "    pages, so we can scrape each of those iteratively.\n",
    "    \n",
    "    Params\n",
    "    -------\n",
    "    skill_list\n",
    "        A list of all the skills searched for in these pages\n",
    "    \n",
    "    Return\n",
    "    -------\n",
    "    <dict> {course_name: course_link, ...}\n",
    "        course_name: string containing course name\n",
    "        course_link: string containing link to course description page\n",
    "    \"\"\"\n",
    "    html_base = 'https://api.heinz.cmu.edu'\n",
    "    html = urlopen('https://api.heinz.cmu.edu/courses_api/course_list/')\n",
    "    soup = BeautifulSoup(html.read(), \"lxml\")\n",
    "    \n",
    "    names = []\n",
    "    links = []\n",
    "    \n",
    "    # Each course link is contained in a <tr> element with class=\"clickable-row\"\n",
    "    course_rows = soup.findAll('tr', {'class': 'clickable-row'})\n",
    "    for row in course_rows:\n",
    "        names.append(row.find_all('td')[1].string)\n",
    "        links.append(html_base + row.get('data-href'))\n",
    "    \n",
    "    return dict(zip(names, links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_course_description(link):\n",
    "    # Retrieve the html\n",
    "    html = urlopen(link)\n",
    "    soup = BeautifulSoup(html.read(), \"lxml\")\n",
    "\n",
    "    # Recover the description text\n",
    "    text = soup.find_all('p')[2].get_text()\n",
    "    clean = text.replace('\\n', '').replace('\\r', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = urlopen('https://api.heinz.cmu.edu/courses_api/course_list/')\n",
    "soup = BeautifulSoup(html.read(), \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "course_rows = bsyc.findAll('tr', {'class': 'clickable-row'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "course_links = get_course_links()\n",
    "course_links['Heinz Journal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = urlopen(course_links['Heinz Journal'])\n",
    "soup = BeautifulSoup(html.read(), \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = soup.find_all('p')[2].get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test it\n",
    "test_list = ['Management', 'software', 'knowledge']\n",
    "results = get_skill_map(test_list)\n",
    "write_to_csv(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
