{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define function that scrapes one page at a time and returns a panda dataframe\n",
    "def scrape_page(page_num):\n",
    "    import requests  # b_soup_1.py\n",
    "    import re\n",
    "    from bs4 import BeautifulSoup\n",
    "    #spoof the user agent to make page scrapable\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
    "#define position search term (one word) and location\n",
    "    position='analyst'\n",
    "    location='Pittsburgh,+PA'\n",
    "    page_str=str(page_num)\n",
    "#url for search query\n",
    "    url='https://www.indeed.com/jobs?q='+position+'&l='+location+'&start='+page_str\n",
    "    search_page=requests.get(url,headers=headers)\n",
    "    search_page\n",
    "    soup=BeautifulSoup(search_page.content,'html.parser')\n",
    "    all_cards=soup.find_all(class_='jobsearch-SerpJobCard row result')\n",
    "#empty list to store job posting urls\n",
    "    job_urls=[]\n",
    "#it seems to be easier to pull posting dates and locations from search page, so do that here\n",
    "    posting_dates=[]\n",
    "    locations=[]\n",
    "#iterate through all_cards, extract job posting urls, posting dates, and locations\n",
    "    for card in all_cards:\n",
    "    #urls\n",
    "        job_id=card['data-jk']\n",
    "        job_url='https://www.indeed.com/viewjob?jk='+job_id\n",
    "        job_urls.append(job_url)\n",
    "    #posting dates\n",
    "    #some jobs don't have posting dates so try them\n",
    "        try:\n",
    "            job_post_date=card.find(class_='date').get_text()\n",
    "        except:\n",
    "            job_post_date=' '\n",
    "        posting_dates.append(job_post_date)\n",
    "    #locations\n",
    "        job_location=card.find(class_='location').get_text()\n",
    "        locations.append(job_location)\n",
    "#locations\n",
    "#lets make this dataframe\n",
    "#declare the empty lists that will become columns\n",
    "    position_name=[]\n",
    "    employer=[]\n",
    "    full_description=[]\n",
    "#in the future, we can develop a list of keywords that we'll \n",
    "  #search postings for, but for now, just return all capitalized\n",
    "   #words besides common words\n",
    "    capital_words=[]\n",
    "    for job_url in job_urls:\n",
    "    #retrieve job page\n",
    "        job_page=requests.get(job_url,headers=headers)\n",
    "    #make job page into BeautifulSoup object\n",
    "        job_soup=BeautifulSoup(job_page.content,'html.parser')\n",
    "    #retrieve the job title\n",
    "        job_position=job_soup.find(class_='icl-u-xs-mb--xs').get_text()\n",
    "    #retrieve company name from job rating line on page\n",
    "        job_rating_line=job_soup.find(class_='jobsearch-InlineCompanyRating')\n",
    "        job_employer=job_rating_line.find(class_='icl-u-lg-mr--sm').get_text()\n",
    "    #retrieve full job description\n",
    "        job_descrip=job_soup.find(class_='jobsearch-JobComponent-description').get_text()\n",
    "    #replace line breaks in description with spaces\n",
    "        job_descrip.replace('\\n',' ')\n",
    "    #find capitalized words in job description\n",
    "        capitals=re.findall('([A-Z][a-z]+)', job_descrip)\n",
    "    #replace commonly capitalized words\n",
    "        for x in ['The','Our','A','As','On','Using','We', 'In', 'Some']:\n",
    "            try:\n",
    "                capitals[:]=(value for value in capitals if value!=x)\n",
    "            except:\n",
    "                capitals\n",
    "    #make capitals into a set instead of a list\n",
    "        capitals=set(capitals)\n",
    "        capital_string=', '.join(capitals)\n",
    "    #append all of the items created to the empty lists from last step\n",
    "        position_name.append(job_position)\n",
    "        employer.append(job_employer)\n",
    "        full_description.append(job_descrip)\n",
    "        capital_words.append(capitals)\n",
    "        \n",
    "#store results in dictionary, then data frame\n",
    "import pandas as pd\n",
    "result_dict={'Job Title':position_name,\n",
    "            'Employer':employer,\n",
    "            'Location':locations,\n",
    "            'Posting Date':posting_dates, \n",
    "            'Posting Url':job_urls,\n",
    "            'Full Job Description':full_description,\n",
    "            'Keywords':capital_words\n",
    "            }\n",
    "result_frame=pd.DataFrame(result_dict)\n",
    "return(resu)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
